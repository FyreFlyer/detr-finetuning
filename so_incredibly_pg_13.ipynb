{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CSE 4095 Vision Transformer Project**\n",
        "\n",
        "There is a lot of opportunity to automate the process of detecting violence through our surveilance cameras. Our goal was to find if there was a way to automate the discovery of people carrying guns in public to better help police understand where to focus their attention in times of danger.\n",
        "\n",
        "We used a transformer model since we believed they would provide some benefits over other methods. Namely the attention mechanism could have a better time understanding the context of the environment. The DETR model is an encoder-decoder transformer with a convolutional backbone. The encoder extract meaningful representations of the image, and the decoder generates predictions on if there is a human with a gun based on these representations.\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "![](https://cdn-thumbnails.huggingface.co/social-thumbnails/models/facebook/detr-resnet-50.png)"
      ],
      "metadata": {
        "id": "m583fq8Fd405"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the Dataset\n",
        "We use an API key to load our firearm object detection dataset from a Roboflow workspace."
      ],
      "metadata": {
        "id": "SzXItCopKZJf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvhC9kYxdZ6G",
        "outputId": "4b48f2a0-1d24-4728-d473-619e2670f462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.27)\n",
            "Requirement already satisfied: certifi==2023.7.22 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2023.7.22)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.25.2)\n",
            "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.8.0.74)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.4.27)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.51.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading Dataset Version Zip in Firearm-Detection-1 to coco:: 100%|██████████| 72019/72019 [00:03<00:00, 22420.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to Firearm-Detection-1 in coco:: 100%|██████████| 1192/1192 [00:00<00:00, 2872.57it/s]\n"
          ]
        }
      ],
      "source": [
        "# load in the coco data from roboflow for person detection\n",
        "# !pip install roboflow\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"Hx4hLh7HJ91dQsTehTZK\")\n",
        "project = rf.workspace(\"playground-wxriu\").project(\"firearm-detection-5ioov\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"coco\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arranging files\n",
        "The following block of code arranges all images and annotations in a way that is convenient for the DETR framework"
      ],
      "metadata": {
        "id": "DN1AaZlpyBQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/data/custom/annotations/\n",
        "\n",
        "!mv /content/Firearm-Detection-1/train/_annotations.coco.json /content/data/custom/annotations/custom_train.json\n",
        "!mv /content/Firearm-Detection-1/test/_annotations.coco.json /content/data/custom/annotations/custom_val.json\n",
        "\n",
        "!mkdir -p /content/data/custom/train2017/\n",
        "\n",
        "!mv /content/Firearm-Detection-1/train/*.jpg /content/data/custom/train2017/\n",
        "\n",
        "!mkdir -p /content/data/custom/val2017/\n",
        "\n",
        "!mv /content/Firearm-Detection-1/test/*.jpg /content/data/custom/val2017/\n",
        "\n",
        "dataDir='/content/data/custom/'\n",
        "dataType='train2017'\n",
        "annFile='{}annotations/custom_train.json'.format(dataDir)"
      ],
      "metadata": {
        "id": "a8IB5X-ky0f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3815219-5660-43cd-e759-ade898abdd40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/Firearm-Detection-1/train/_annotations.coco.json': No such file or directory\n",
            "mv: cannot stat '/content/Firearm-Detection-1/test/_annotations.coco.json': No such file or directory\n",
            "mv: cannot stat '/content/Firearm-Detection-1/train/*.jpg': No such file or directory\n",
            "mv: cannot stat '/content/Firearm-Detection-1/test/*.jpg': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important imports\n",
        "\n",
        "The following code blocks cover all remaining imports for the code."
      ],
      "metadata": {
        "id": "qAkhzSWMyPUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import requests\n",
        "%matplotlib inline\n",
        "import pycocotools.coco as coco\n",
        "from pycocotools.coco import COCO\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab\n",
        "\n",
        "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
        "print(torch.__version__, torch.cuda.is_available())\n",
        "torch.set_grad_enabled(False);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFbyX1W1HEEr",
        "outputId": "fca97cdd-d349-4c9c-c84c-159daa6aa44e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.1+cu121 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function setup\n",
        "\n",
        "Sets up important functions for the code."
      ],
      "metadata": {
        "id": "ESxVfSLR1kdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# standard PyTorch mean-std input image normalization\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# for output bounding box post-processing\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "def filter_bboxes_from_outputs(outputs,\n",
        "                               threshold=0.7):\n",
        "  # keep only predictions with confidence above threshold\n",
        "  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "  keep = probas.max(-1).values > threshold\n",
        "  probas_to_keep = probas[keep]\n",
        "  # convert boxes from [0; 1] to image scales\n",
        "  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
        "  return probas_to_keep, bboxes_scaled"
      ],
      "metadata": {
        "id": "wDWhpmzYpyVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "\n",
        "!rm -rf detr\n",
        "!git clone https://github.com/woctezuma/detr.git\n",
        "\n",
        "%cd detr/\n",
        "\n",
        "!git checkout finetune"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCC-OGRU1ipK",
        "outputId": "def847d3-47a0-47a2-fb33-90edd9eb65e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'detr'...\n",
            "remote: Enumerating objects: 265, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 265 (delta 4), reused 6 (delta 3), pack-reused 254\u001b[K\n",
            "Receiving objects: 100% (265/265), 325.43 KiB | 7.40 MiB/s, done.\n",
            "Resolving deltas: 100% (127/127), done.\n",
            "/content/detr\n",
            "Already on 'finetune'\n",
            "Your branch is up to date with 'origin/finetune'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get pretrained weights\n",
        "checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n",
        "            map_location='cpu',\n",
        "            check_hash=True)\n",
        "\n",
        "# Remove class weights\n",
        "del checkpoint[\"model\"][\"class_embed.weight\"]\n",
        "del checkpoint[\"model\"][\"class_embed.bias\"]\n",
        "\n",
        "# Save\n",
        "torch.save(checkpoint,\n",
        "           'detr-r50_no-class-head.pth')"
      ],
      "metadata": {
        "id": "4tCdrYMm1nVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize COCO api for instance annotations\n",
        "coco=COCO(annFile)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "YkOrVHgf1yd3",
        "outputId": "1188981f-15a0-4eff-fd98-8647bb044cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data/custom/annotations/custom_train.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-ef7217ac4bcd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initialize COCO api for instance annotations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoco\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOCO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pycocotools/coco.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, annotation_file)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loading annotations into memory...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotation_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'annotation file format {} not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/custom/annotations/custom_train.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# display COCO categories and supercategories\n",
        "cats = coco.loadCats(coco.getCatIds())\n",
        "nms=[cat['name'] for cat in cats]\n",
        "print('Categories: {}'.format(nms))\n",
        "nms = set([cat['supercategory'] for cat in cats])\n",
        "print('Super-categories: {}'.format(nms))"
      ],
      "metadata": {
        "id": "Wk26pe8310eD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and display image\n",
        "catIds = coco.getCatIds(catNms=['gun']);\n",
        "imgIds = coco.getImgIds(catIds=catIds);\n",
        "\n",
        "img_id = imgIds[np.random.randint(0,len(imgIds))]\n",
        "print('Image n°{}'.format(img_id))\n",
        "\n",
        "img = coco.loadImgs(img_id)[0]\n",
        "\n",
        "img_name = '%s/%s/%s'%(dataDir, dataType, img['file_name'])\n",
        "print('Image name: {}'.format(img_name))\n",
        "\n",
        "I = io.imread(img_name)\n",
        "plt.figure()\n",
        "plt.imshow(I)"
      ],
      "metadata": {
        "id": "Ourg4kx713rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds)\n",
        "anns = coco.loadAnns(annIds)\n",
        "plt.imshow(I)\n",
        "coco.showAnns(anns, draw_bbox=True)"
      ],
      "metadata": {
        "id": "bzs_sjAz3qzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_class_index = 0\n",
        "num_classes = 2\n",
        "finetuned_classes = [\n",
        "    'Firearm',\n",
        "    'gun'\n",
        "]\n",
        "\n",
        "print('First class index: {}'.format(first_class_index))\n",
        "print('Parameter num_classes: {}'.format(num_classes))\n",
        "print('Fine-tuned classes: {}'.format(finetuned_classes))"
      ],
      "metadata": {
        "id": "1XFfNxn13645"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/detr/"
      ],
      "metadata": {
        "id": "flOQHmAt4ls_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING SAVED MODEL WEIGHTS!!"
      ],
      "metadata": {
        "id": "ouMee2sOXG-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('facebookresearch/detr',\n",
        "                       'detr_resnet50',\n",
        "                       pretrained=False,\n",
        "                       num_classes=num_classes)\n",
        "\n",
        "checkpoint = torch.load('checkpoint.pth',\n",
        "                        map_location='cpu')\n",
        "\n",
        "model.load_state_dict(checkpoint['model'],\n",
        "                      strict=False)\n",
        "\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "qQQT57ebXGhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...TO FINE-TUNE"
      ],
      "metadata": {
        "id": "SrmOIbySXKAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py \\\n",
        "  --dataset_file \"custom\" \\\n",
        "  --coco_path \"/content/data/custom/\" \\\n",
        "  --output_dir \"outputs\" \\\n",
        "  --resume \"detr-r50_no-class-head.pth\" \\\n",
        "  --num_classes $num_classes \\\n",
        "  --epochs 15"
      ],
      "metadata": {
        "id": "OTzJNcPI4pTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.hub.load('facebookresearch/detr',\n",
        "                       'detr_resnet50',\n",
        "                       pretrained=False,\n",
        "                       num_classes=num_classes)\n",
        "\n",
        "checkpoint = torch.load('outputs/checkpoint.pth',\n",
        "                        map_location='cpu')\n",
        "\n",
        "model.load_state_dict(checkpoint['model'],\n",
        "                      strict=False)\n",
        "\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "8XzK1HMg5J-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "def plot_finetuned_results(pil_img, prob=None, boxes=None):\n",
        "    plt.figure(figsize=(16,10))\n",
        "    plt.imshow(pil_img)\n",
        "    ax = plt.gca()\n",
        "    colors = COLORS * 100\n",
        "    if prob is not None and boxes is not None:\n",
        "      for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
        "          ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                    fill=False, color=c, linewidth=3))\n",
        "          cl = p.argmax()\n",
        "          text = f'{finetuned_classes[cl]}: {p[cl]:0.2f}'\n",
        "          ax.text(xmin, ymin, text, fontsize=15,\n",
        "                  bbox=dict(facecolor='yellow', alpha=0.5))\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def run_worflow(my_image, my_model):\n",
        "  # mean-std normalize the input image (batch-size: 1)\n",
        "  img = transform(my_image).unsqueeze(0)\n",
        "\n",
        "  # propagate through the model\n",
        "  outputs = my_model(img)\n",
        "  for threshold in [0.9,]:\n",
        "    probas_to_keep, bboxes_scaled = filter_bboxes_from_outputs(outputs,\n",
        "                                                              threshold=threshold)\n",
        "    plot_finetuned_results(my_image,\n",
        "                           probas_to_keep,\n",
        "                           bboxes_scaled)"
      ],
      "metadata": {
        "id": "RHS9nNsr5MBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_name = '/content/data/custom/train2017/100.rf.21be8bfacac0dc677f00640161e8643e.jpg'\n",
        "im = Image.open(img_name)\n",
        "run_worflow(im,\n",
        "            model)"
      ],
      "metadata": {
        "id": "SYuV2kcj5RKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_name = '/content/data/custom/val2017/real-video-360-.rf.21daaa9da4acc88a0dbc2f69eaa74674.jpg'\n",
        "im = Image.open(img_name)\n",
        "\n",
        "run_worflow(im,\n",
        "            model)"
      ],
      "metadata": {
        "id": "uDaMY48P5TNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the model"
      ],
      "metadata": {
        "id": "rklfpH3wKFcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ],
      "metadata": {
        "id": "KKhwf_oyKE5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Future Work**\n",
        "We plan to train this model again on for longer and with a better dataset(s)."
      ],
      "metadata": {
        "id": "DpMyT3rhws4Z"
      }
    }
  ]
}